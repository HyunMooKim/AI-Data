{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsFO+5VskObDeDBp3q0I0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunMooKim/python-basic/blob/main/nltk_top_10_words_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run this first\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMmXn8BtLDp0",
        "outputId": "3e35e5a1-0fbb-45a2-fc79-b081b5fae69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N81qnuUBOK-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf6f484-9df1-4cf4-f165-594237e5e714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank\tWord            Frequency\n",
            "------------------------------\n",
            "1. vulnerability  \t         5\n",
            "2. Odoo           \t         4\n",
            "3. allows         \t         3\n",
            "4. attackers      \t         3\n",
            "5. crafted        \t         3\n",
            "6. path           \t         3\n",
            "7. code           \t         3\n",
            "8. execution      \t         3\n",
            "9. scripting      \t         3\n",
            "10. Server         \t         3\n"
          ]
        }
      ],
      "source": [
        "# I stored html files names as CVE1.html ~ CVE10.html\n",
        "# If you want to run the code, you have to upload html file as name of CVE1.html ~ CVE10.html to COLAB session storage\n",
        "'''\n",
        "10개의 파일에서 가장 많이 등장하는 단어를 추출하는 코드\n",
        "'''\n",
        "# Define a list of stopwords (meaningless words)\n",
        "# Like: 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# dictionary to store the word frequency counts\n",
        "word_freq = {}\n",
        "\n",
        "# lists of POS tags for nouns and verbs to only accept noun and verb\n",
        "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "\n",
        "# for Loop through each file and count the word frequency\n",
        "for i in range(1, 11):\n",
        "    filename = \"CVE{}.html\".format(i)\n",
        "    #When you use the 'with' statement, Python automatically closes the file\n",
        "    with open(filename, \"rt\") as f:\n",
        "        text = f.read()\n",
        "        '''\n",
        "        If you want to remove HTML tags you can use \n",
        "        text = re.sub('<[^>]*>', '', text)\n",
        "        \n",
        "        '''\n",
        "        # Use re to find the text within the vuln-description tags (description part)\n",
        "        text = re.findall(r'<p data-testid=\"vuln-description\">(.*?)</p>', text, flags=re.DOTALL)\n",
        "        # Tokenize the text into words\n",
        "        words = nltk.tokenize.word_tokenize(text[0])\n",
        "        '''\n",
        "        If you don't want to differentiate between uppercase and lowercase words, use this code below:\n",
        "        words = nltk.tokenize.word_tokenize(text[0].lower())\n",
        "        '''\n",
        "        # Filter out stopwords and punctuation\n",
        "        words = [word for word in words if word.isalpha() and word not in stopwords]\n",
        "        # Tag each word with its POS tag\n",
        "        pos_tags = nltk.pos_tag(words)\n",
        "        # Count the frequency of each noun and verb\n",
        "        for word, pos in pos_tags:\n",
        "            if pos in noun_tags or pos in verb_tags:\n",
        "                if word in word_freq:\n",
        "                    word_freq[word] += 1\n",
        "                else:\n",
        "                    word_freq[word] = 1\n",
        "# Sort the dictionary by value in descending order and get the top 10 words\n",
        "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "\n",
        "# Print the top 10 words and their frequency\n",
        "print(\"Rank\\tWord\".ljust(20) + \"Frequency\".rjust(10))\n",
        "print(\"-\" * 30)\n",
        "for i, (word, freq) in enumerate(sorted_word_freq):\n",
        "    print(\"{}. {}\\t{}\".format(i + 1, word.ljust(15), str(freq).rjust(10)))"
      ]
    }
  ]
}